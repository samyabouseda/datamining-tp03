{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> TP3 - 01 Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective of TP\n",
    "\n",
    "In this TP you will practice your skills of independent and by now (fairly) experienced data analyst.  \n",
    "Being able to work independently and to build on your knowledge and experience to adapt to new algorithms is an essential part of the work of a data analyst.\n",
    "There is **no single best** algorithm that you could learn in a data analysis class and live with it for the rest of your life. \n",
    "Rahter the opposite, there are many algorithms (tens or even hundreds) which all have their pros and cons. \n",
    "No class can cover them all. But a class such as ours can teach you the basic principles on which you should build to be able to use new algorithms that you never even heard before.\n",
    "\n",
    "In this TP. you will work with two new algorithms:\n",
    "* you will implement on your own **Naive Bayes classifier** (*Course 11 - 01 Naive Bayes algoritm*)\n",
    "* you will use the scikit-learn implementation of the **logistic regression** (*Course 12 - 02 Logistic regression*) and use the official documentation together with any other information you can find (google) to understand how to use it\n",
    "\n",
    "You will reuse your work from TP2 on the full **supervised learning pipeline** to train, pick and evaluate your models.\n",
    "\n",
    "### Recommendation:\n",
    "As always, the code you will develop in this TP is to be re-used later (in the exam).  \n",
    "Therefore we recommend you try to make it clear (use comments, when printing say what you print) so that next time it is easier for you to remember what it does.  \n",
    "Also, try to make the code generic so that it can be easilly used for different datasets.   \n",
    "Try to automate as much as possible so that the code does not require too much of your attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing TP2 \n",
    "\n",
    "## Dataset\n",
    "\n",
    "You will be workig with the same cars dataset as in TP1 and TP2.  \n",
    "Each group shall be using the same `brands` as in TP1 and TP2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>vehicleType</th>\n",
       "      <th>yearOfRegistration</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>powerPS</th>\n",
       "      <th>model</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>brand</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "      <th>fast_sale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11400.0</td>\n",
       "      <td>limousine</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>manuell</td>\n",
       "      <td>175.0</td>\n",
       "      <td>mondeo</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>diesel</td>\n",
       "      <td>ford</td>\n",
       "      <td>nein</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4100.0</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>manuell</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1_reihe</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>nein</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>888.0</td>\n",
       "      <td>kombi</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>manuell</td>\n",
       "      <td>115.0</td>\n",
       "      <td>mondeo</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>ford</td>\n",
       "      <td>nein</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13700.0</td>\n",
       "      <td>bus</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>manuell</td>\n",
       "      <td>86.0</td>\n",
       "      <td>roomster</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>skoda</td>\n",
       "      <td>nein</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4299.0</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>manuell</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2_reihe</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>nein</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price vehicleType  yearOfRegistration  gearbox  powerPS     model  \\\n",
       "2  11400.0   limousine              2010.0  manuell    175.0    mondeo   \n",
       "4   4100.0  kleinwagen              2009.0  manuell     68.0   1_reihe   \n",
       "6    888.0       kombi              2000.0  manuell    115.0    mondeo   \n",
       "7  13700.0         bus              2012.0  manuell     86.0  roomster   \n",
       "9   4299.0  kleinwagen              2010.0  manuell     75.0   2_reihe   \n",
       "\n",
       "   kilometer fuelType    brand notRepairedDamage  fast_sale  \n",
       "2   125000.0   diesel     ford              nein      False  \n",
       "4    90000.0   benzin  peugeot              nein      False  \n",
       "6   150000.0   benzin     ford              nein       True  \n",
       "7     5000.0   benzin    skoda              nein       True  \n",
       "9   125000.0   benzin  peugeot              nein      False  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset and extract our part\n",
    "import pandas as pd\n",
    "\n",
    "# Reading csv file\n",
    "autos = pd.read_csv('autos.csv',encoding='latin-1')\n",
    "\n",
    "# Extracting the relevant part for our group\n",
    "only_specific_brands = autos.brand.isin(['renault', 'peugeot', 'skoda', 'citroen', 'ford'])\n",
    "autos = autos[only_specific_brands]\n",
    "autos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Remember that after loading the dataset, there are several preprocessing steps you need to do before training the algorithms.\n",
    "You already did all the necessary pre-processing steps in TP2 so you can simply reuse them.   \n",
    "**Important note:** While in practice the step *'check and clean your data'* is super important, for our class (this TP and exam) consider the data to be checked and clean already so you can skip it.\n",
    "\n",
    "Remember to comment in your code the pre-processing steps you do (this is important for you or any other user of your code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we are changing the column names:\n",
    "\n",
    "- We changed the style of writing the column names from camelcase to snakecase. This change will make column names easier to read, and we won't have to remember which letter is capitalized.\n",
    "- We reworded some of the column names in order for them to be more descriptive and obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>vehicle_type</th>\n",
       "      <th>registration_year</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>power_ps</th>\n",
       "      <th>model</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>unrepaired_damage</th>\n",
       "      <th>fast_sale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11400.0</td>\n",
       "      <td>limousine</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>manuell</td>\n",
       "      <td>175.0</td>\n",
       "      <td>mondeo</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>diesel</td>\n",
       "      <td>ford</td>\n",
       "      <td>nein</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4100.0</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>manuell</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1_reihe</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>nein</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>888.0</td>\n",
       "      <td>kombi</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>manuell</td>\n",
       "      <td>115.0</td>\n",
       "      <td>mondeo</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>ford</td>\n",
       "      <td>nein</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13700.0</td>\n",
       "      <td>bus</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>manuell</td>\n",
       "      <td>86.0</td>\n",
       "      <td>roomster</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>skoda</td>\n",
       "      <td>nein</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4299.0</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>manuell</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2_reihe</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>nein</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price vehicle_type  registration_year  gearbox  power_ps     model  \\\n",
       "2  11400.0    limousine             2010.0  manuell     175.0    mondeo   \n",
       "4   4100.0   kleinwagen             2009.0  manuell      68.0   1_reihe   \n",
       "6    888.0        kombi             2000.0  manuell     115.0    mondeo   \n",
       "7  13700.0          bus             2012.0  manuell      86.0  roomster   \n",
       "9   4299.0   kleinwagen             2010.0  manuell      75.0   2_reihe   \n",
       "\n",
       "   kilometer fuel_type    brand unrepaired_damage  fast_sale  \n",
       "2   125000.0    diesel     ford              nein      False  \n",
       "4    90000.0    benzin  peugeot              nein      False  \n",
       "6   150000.0    benzin     ford              nein       True  \n",
       "7     5000.0    benzin    skoda              nein       True  \n",
       "9   125000.0    benzin  peugeot              nein      False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing the column names\n",
    "autos.columns = ['price', 'vehicle_type', 'registration_year', 'gearbox', 'power_ps', 'model', \n",
    "                 'kilometer', 'fuel_type', 'brand', 'unrepaired_damage', 'fast_sale']\n",
    "autos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we are going to convert some columns data type:\n",
    "\n",
    "- We are changing data type in order to facilitate the data processing (classifying the data in numeric or categorical)\n",
    "- We are converting column 'unrepaired_damage' from object to boolean type\n",
    "- We are converting column 'registration_year' from float to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28401 entries, 2 to 166073\n",
      "Data columns (total 11 columns):\n",
      "price                28401 non-null float64\n",
      "vehicle_type         28401 non-null object\n",
      "registration_year    28401 non-null int64\n",
      "gearbox              28401 non-null object\n",
      "power_ps             28401 non-null float64\n",
      "model                28401 non-null object\n",
      "kilometer            28401 non-null float64\n",
      "fuel_type            28401 non-null object\n",
      "brand                28401 non-null object\n",
      "unrepaired_damage    28401 non-null bool\n",
      "fast_sale            28401 non-null bool\n",
      "dtypes: bool(2), float64(3), int64(1), object(5)\n",
      "memory usage: 2.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>registration_year</th>\n",
       "      <th>power_ps</th>\n",
       "      <th>kilometer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28401.000000</td>\n",
       "      <td>28401.000000</td>\n",
       "      <td>28401.000000</td>\n",
       "      <td>28401.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4177.515017</td>\n",
       "      <td>2003.965565</td>\n",
       "      <td>102.402979</td>\n",
       "      <td>121472.307313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4680.629533</td>\n",
       "      <td>5.872460</td>\n",
       "      <td>40.664873</td>\n",
       "      <td>39816.529262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1923.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1199.000000</td>\n",
       "      <td>2001.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5400.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>73500.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>952.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price  registration_year      power_ps      kilometer\n",
       "count  28401.000000       28401.000000  28401.000000   28401.000000\n",
       "mean    4177.515017        2003.965565    102.402979  121472.307313\n",
       "std     4680.629533           5.872460     40.664873   39816.529262\n",
       "min        1.000000        1923.000000      2.000000    5000.000000\n",
       "25%     1199.000000        2001.000000     75.000000  100000.000000\n",
       "50%     2500.000000        2004.000000    101.000000  150000.000000\n",
       "75%     5400.000000        2008.000000    122.000000  150000.000000\n",
       "max    73500.000000        2016.000000    952.000000  150000.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting column 'unrepaired_damage' from object to boolean type\n",
    "autos['unrepaired_damage'] = (autos['unrepaired_damage']\n",
    "                 .str.replace('nein','')\n",
    "                 .str.replace('ja','True')\n",
    "                  .astype(bool)\n",
    "                 )\n",
    "# Converting column 'registration_year' from float to int.\n",
    "autos['registration_year'] = (autos['registration_year'].astype(int))\n",
    "autos.info()\n",
    "autos.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we are goint to decide what is output (target) and what is input:\n",
    "- This steps depends what we are trying to predict. Here we want to predict if a car will be sold fastly (fast_sale = True). Therefore we need to select the 'fast_sale' data as output data. \n",
    "- The other attributes will be the input data (the data that we will use in order to predict if a car will be sold fastly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>vehicle_type</th>\n",
       "      <th>registration_year</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>power_ps</th>\n",
       "      <th>model</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>unrepaired_damage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11400.0</td>\n",
       "      <td>limousine</td>\n",
       "      <td>2010</td>\n",
       "      <td>manuell</td>\n",
       "      <td>175.0</td>\n",
       "      <td>mondeo</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>diesel</td>\n",
       "      <td>ford</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4100.0</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2009</td>\n",
       "      <td>manuell</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1_reihe</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>888.0</td>\n",
       "      <td>kombi</td>\n",
       "      <td>2000</td>\n",
       "      <td>manuell</td>\n",
       "      <td>115.0</td>\n",
       "      <td>mondeo</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>ford</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13700.0</td>\n",
       "      <td>bus</td>\n",
       "      <td>2012</td>\n",
       "      <td>manuell</td>\n",
       "      <td>86.0</td>\n",
       "      <td>roomster</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>skoda</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4299.0</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2010</td>\n",
       "      <td>manuell</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2_reihe</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price vehicle_type  registration_year  gearbox  power_ps     model  \\\n",
       "2  11400.0    limousine               2010  manuell     175.0    mondeo   \n",
       "4   4100.0   kleinwagen               2009  manuell      68.0   1_reihe   \n",
       "6    888.0        kombi               2000  manuell     115.0    mondeo   \n",
       "7  13700.0          bus               2012  manuell      86.0  roomster   \n",
       "9   4299.0   kleinwagen               2010  manuell      75.0   2_reihe   \n",
       "\n",
       "   kilometer fuel_type    brand  unrepaired_damage  \n",
       "2   125000.0    diesel     ford              False  \n",
       "4    90000.0    benzin  peugeot              False  \n",
       "6   150000.0    benzin     ford              False  \n",
       "7     5000.0    benzin    skoda              False  \n",
       "9   125000.0    benzin  peugeot              False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    False\n",
       "4    False\n",
       "6     True\n",
       "7     True\n",
       "9    False\n",
       "Name: fast_sale, dtype: bool"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# output dataframe\n",
    "out_autos = autos['fast_sale']\n",
    "# input dataframe\n",
    "in_autos = autos.iloc[:,[0,1,2,3,4,5,6,7,8,9]]\n",
    "\n",
    "print('Inputs')\n",
    "display(in_autos.head())\n",
    "print('Outputs')\n",
    "display(out_autos.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we convert categorical to numeric (one-hot encoding)\n",
    "\n",
    "- The algorithms we use (except the deciscion tree) cannot operate on label data directly. They require all input variables and output variables to be numeric. Therefore we need to convert categorical data to a numerical form.\n",
    "- Since no ordinal relationship exists in the categorical variables in our dataset (and we don't want to allow the model to assume a natural ordering between categories), we use the one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input data\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>vehicle_type</th>\n",
       "      <th>registration_year</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>power_ps</th>\n",
       "      <th>model</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>unrepaired_damage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11400.0</td>\n",
       "      <td>limousine</td>\n",
       "      <td>2010</td>\n",
       "      <td>manuell</td>\n",
       "      <td>175.0</td>\n",
       "      <td>mondeo</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>diesel</td>\n",
       "      <td>ford</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4100.0</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2009</td>\n",
       "      <td>manuell</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1_reihe</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>888.0</td>\n",
       "      <td>kombi</td>\n",
       "      <td>2000</td>\n",
       "      <td>manuell</td>\n",
       "      <td>115.0</td>\n",
       "      <td>mondeo</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>ford</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13700.0</td>\n",
       "      <td>bus</td>\n",
       "      <td>2012</td>\n",
       "      <td>manuell</td>\n",
       "      <td>86.0</td>\n",
       "      <td>roomster</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>skoda</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4299.0</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2010</td>\n",
       "      <td>manuell</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2_reihe</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price vehicle_type  registration_year  gearbox  power_ps     model  \\\n",
       "2  11400.0    limousine               2010  manuell     175.0    mondeo   \n",
       "4   4100.0   kleinwagen               2009  manuell      68.0   1_reihe   \n",
       "6    888.0        kombi               2000  manuell     115.0    mondeo   \n",
       "7  13700.0          bus               2012  manuell      86.0  roomster   \n",
       "9   4299.0   kleinwagen               2010  manuell      75.0   2_reihe   \n",
       "\n",
       "   kilometer fuel_type    brand  unrepaired_damage  \n",
       "2   125000.0    diesel     ford              False  \n",
       "4    90000.0    benzin  peugeot              False  \n",
       "6   150000.0    benzin     ford              False  \n",
       "7     5000.0    benzin    skoda              False  \n",
       "9   125000.0    benzin  peugeot              False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Numerical input data\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>registration_year</th>\n",
       "      <th>power_ps</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>unrepaired_damage</th>\n",
       "      <th>vehicle_type_andere</th>\n",
       "      <th>vehicle_type_bus</th>\n",
       "      <th>vehicle_type_cabrio</th>\n",
       "      <th>vehicle_type_coupe</th>\n",
       "      <th>vehicle_type_kleinwagen</th>\n",
       "      <th>...</th>\n",
       "      <th>fuel_type_cng</th>\n",
       "      <th>fuel_type_diesel</th>\n",
       "      <th>fuel_type_elektro</th>\n",
       "      <th>fuel_type_hybrid</th>\n",
       "      <th>fuel_type_lpg</th>\n",
       "      <th>brand_citroen</th>\n",
       "      <th>brand_ford</th>\n",
       "      <th>brand_peugeot</th>\n",
       "      <th>brand_renault</th>\n",
       "      <th>brand_skoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11400.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>175.0</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4100.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>68.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>888.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13700.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>86.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4299.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>75.0</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     price  registration_year  power_ps  kilometer  unrepaired_damage  \\\n",
       "2  11400.0               2010     175.0   125000.0              False   \n",
       "4   4100.0               2009      68.0    90000.0              False   \n",
       "6    888.0               2000     115.0   150000.0              False   \n",
       "7  13700.0               2012      86.0     5000.0              False   \n",
       "9   4299.0               2010      75.0   125000.0              False   \n",
       "\n",
       "   vehicle_type_andere  vehicle_type_bus  vehicle_type_cabrio  \\\n",
       "2                    0                 0                    0   \n",
       "4                    0                 0                    0   \n",
       "6                    0                 0                    0   \n",
       "7                    0                 1                    0   \n",
       "9                    0                 0                    0   \n",
       "\n",
       "   vehicle_type_coupe  vehicle_type_kleinwagen     ...       fuel_type_cng  \\\n",
       "2                   0                        0     ...                   0   \n",
       "4                   0                        1     ...                   0   \n",
       "6                   0                        0     ...                   0   \n",
       "7                   0                        0     ...                   0   \n",
       "9                   0                        1     ...                   0   \n",
       "\n",
       "   fuel_type_diesel  fuel_type_elektro  fuel_type_hybrid  fuel_type_lpg  \\\n",
       "2                 1                  0                 0              0   \n",
       "4                 0                  0                 0              0   \n",
       "6                 0                  0                 0              0   \n",
       "7                 0                  0                 0              0   \n",
       "9                 0                  0                 0              0   \n",
       "\n",
       "   brand_citroen  brand_ford  brand_peugeot  brand_renault  brand_skoda  \n",
       "2              0           1              0              0            0  \n",
       "4              0           0              1              0            0  \n",
       "6              0           1              0              0            0  \n",
       "7              0           0              0              0            1  \n",
       "9              0           0              1              0            0  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Original input data')\n",
    "print('-------------------')\n",
    "display(in_autos.head())\n",
    "\n",
    "# Create one-hot encoding\n",
    "in_long = pd.get_dummies(in_autos)\n",
    "print('\\n Numerical input data')\n",
    "print('-------------------')\n",
    "display(in_long.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we normalize (bring all variables to same scale [0,1])\n",
    "\n",
    "-  We need to normalize the data to ensure that all sources are treated equally, and that data-availability bias (and its corresponding misrepresentation of the data universe) is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximums\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "price                 73500.0\n",
       "registration_year      2016.0\n",
       "power_ps                952.0\n",
       "kilometer            150000.0\n",
       "unrepaired_damage         1.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Minimums\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "price                   1.0\n",
       "registration_year    1923.0\n",
       "power_ps                2.0\n",
       "kilometer            5000.0\n",
       "unrepaired_damage       0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " New maximums\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "price                1.0\n",
       "registration_year    1.0\n",
       "power_ps             1.0\n",
       "kilometer            1.0\n",
       "unrepaired_damage    1.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " New minimums\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "price                0.0\n",
       "registration_year    0.0\n",
       "power_ps             0.0\n",
       "kilometer            0.0\n",
       "unrepaired_damage    0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# maximum of each attribute\n",
    "maxs = in_long.max(axis=0) # max accross rows (axis=0)\n",
    "\n",
    "print('Maximums')\n",
    "print('----------')\n",
    "display(maxs.head())\n",
    "\n",
    "# minimum of each attribute\n",
    "mins = in_long.min(axis=0) # max accross rows (axis=0)\n",
    "\n",
    "print('\\n Minimums')\n",
    "print('----------')\n",
    "display(mins.head())\n",
    "\n",
    "# min-max normalization\n",
    "norm_in = (in_long - mins) / (maxs-mins)\n",
    "\n",
    "# new maximum of each attribute\n",
    "maxs = norm_in.max(axis=0) # max accross rows (axis=0)\n",
    "\n",
    "print('\\n New maximums')\n",
    "print('----------')\n",
    "display(maxs.head())\n",
    "\n",
    "# new minimum of each attribute\n",
    "mins = norm_in.min(axis=0) # max accross rows (axis=0)\n",
    "\n",
    "print('\\n New minimums')\n",
    "print('----------')\n",
    "display(mins.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As of this step, no more use of original data and only use preprocessed\n",
    "\n",
    "In our case above: `out_autos` and `norm_in`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for model evaluation and hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splits for model evaluation (training and testing)\n",
    "\n",
    "You have already created a code for this in TP2. In this TP and the exam we will make the evaluation procedure somewhat simpler. Because our datasets are generally rather big, we do not need to repeat the hold-out several times. Instead we will use a **single hold-out** method. That is, we will split the data to training and test (hold-out) datasets only once. In result, we will train only one final model and evaluate the model accuracy only over a single test set.  \n",
    "Remember that the **accuracy over the test data serves as an estimate of the generalization accuracy** and that there is a relation between the confidence we can have in our estimate and the number of samples we have in the test set. A reasonble split to train vs test instances is 2/3 vs 1/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (18934, 67)\n",
      "test (9467, 67)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "\n",
    "out_train, out_test, in_train, in_test = train_test_split(out_autos, norm_in, \n",
    "                                                          train_size=2/3, \n",
    "                                                          test_size=1/3, \n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=random.randint(1, 10000))\n",
    "\n",
    "print(\"train\",in_train.shape)\n",
    "print(\"test\",in_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splits for hyper-parameter tuning\n",
    "\n",
    "Again, you have already created a code for this in TP2 and we will re-use the same procedure (here and in the exam): **use 5-folds inner cross validation** to discover the best values of the hyper-parameters.\n",
    "\n",
    "Remember that once you find the best hyper-parameter values, you should re-train your model with this hyper-parameter value fixed over the whole training set.\n",
    "\n",
    "You then evaluate this final model by comparing its predictions over the test set (hold-out set never used in training) to the true values and establishing the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultClassifier:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.majority_class = None\n",
    "        \n",
    "    def fit(self, in_train, out_train):\n",
    "        # get counts per class\n",
    "        class_counts = out_train.value_counts()\n",
    "        self.majority_class = class_counts.idxmax()\n",
    "        \n",
    "    def predict(self, out_test):\n",
    "        # predict\n",
    "        pred_test = out_test.copy()\n",
    "        pred_test[:] = self.majority_class\n",
    "        return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from sklearn import neighbors, tree, linear_model, naive_bayes\n",
    "    \n",
    "    \n",
    "class ClassifierCallbacks(Enum):\n",
    "    \"\"\"A class containing all the callbacks for the Classifiers\"\"\"\n",
    "    \n",
    "    def clf_neighbors(self, hyper_param):\n",
    "        return neighbors.KNeighborsClassifier(n_neighbors=hyper_param)\n",
    "\n",
    "    def clf_tree(self, hyper_param):\n",
    "        return tree.DecisionTreeClassifier(max_leaf_nodes=hyper_param)\n",
    "    \n",
    "    def clf_default(hyper_param):\n",
    "        return DefaultClassifier()\n",
    "    \n",
    "    def clf_naive_bayes(hyper_param): #need to pass zero\n",
    "        return naive_bayes.GaussianNB()\n",
    "    \n",
    "    def clf_log_regression(hyper_param):\n",
    "        return linear_model.LogisticRegression(C=hyper_param, random_state=123, solver=\"liblinear\")\n",
    "    \n",
    "    K_NEIGHBORS = clf_neighbors\n",
    "    DECISION_TREE = clf_tree\n",
    "    DEFAULT = clf_default\n",
    "    NAIVE_BAYES = clf_naive_bayes\n",
    "    LOGISTIC_REGRESSION = clf_log_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn import neighbors, tree, linear_model\n",
    "import operator\n",
    "\n",
    "\n",
    "class HyperParameterTuner:\n",
    "    \n",
    "    RANDOM_STATE = 123\n",
    "    \n",
    "    def __init__(self, in_data, out_data, hyper_params, \n",
    "                 clf_callback, n_split_outer=5, n_split_inner=3):\n",
    "        self._in_data = in_data\n",
    "        self._out_data = out_data\n",
    "        self._hyper_params = hyper_params\n",
    "        self.n_split_outer = n_split_outer\n",
    "        self.n_split_inner = n_split_inner\n",
    "        self._clf_callback = clf_callback\n",
    "              \n",
    "    def run(self):\n",
    "        self._pre_run_init()\n",
    "        self.cross_validation()\n",
    "        self.calculate_avg_of_hyper_params()\n",
    "        \n",
    "    def _pre_run_init(self):\n",
    "        # Dictionnary holding the cross-validation (cv) scores.\n",
    "        self._cv_scores = self.dict_filled_with_zeros_for_each(self._hyper_params)\n",
    "        self._best_hyper_param_by_fold = dict()\n",
    "    \n",
    "    # We need this function to initiate the cross-validation scores dict with zeros\n",
    "    def dict_filled_with_zeros_for_each(self, keys):\n",
    "        _dict = dict()\n",
    "        for key in keys:\n",
    "            _dict[key] = 0.0\n",
    "        return _dict   \n",
    "            \n",
    "    def cross_validation(self):\n",
    "        k_fold = KFold(n_splits=self.n_split_outer, random_state=self.RANDOM_STATE, shuffle=True)\n",
    "        for train_idx, test_idx in k_fold.split(self._in_data, self._out_data):\n",
    "            in_train = self._in_data.iloc[train_idx]\n",
    "            out_train = self._out_data.iloc[train_idx]\n",
    "            out_test = self._out_data.iloc[test_idx]\n",
    "            self.inner_cross_validation(in_train, out_train)\n",
    "            self._print_best_hyper_param_by_fold()\n",
    "    \n",
    "    def inner_cross_validation(self, in_train, out_train):\n",
    "        for hp in self._hyper_params:\n",
    "            clf = self._clf_callback(hp)\n",
    "            scores = cross_val_score(clf, in_train, out_train, cv=self.n_split_inner)\n",
    "            self._cv_scores[hp] += scores.mean() \n",
    "            self._best_hyper_param_by_fold[hp] = scores.mean() \n",
    "\n",
    "    def _print_best_hyper_param_by_fold(self):\n",
    "        best_hp = max(self._best_hyper_param_by_fold.items(), key=operator.itemgetter(1))[0]\n",
    "        print (\"The optimal hyper-parameter is %d\" % best_hp)\n",
    "                \n",
    "    def calculate_avg_of_hyper_params(self):\n",
    "        for hyper_param, accuracy in self._cv_scores.items():\n",
    "            self._cv_scores[hyper_param] = accuracy / self.n_split_outer\n",
    "    \n",
    "    def get_best_hyper_param(self):\n",
    "        # Get the hyper-param with the highest accuracy in scores dict.\n",
    "        return max(self._cv_scores.items(), key=operator.itemgetter(1))[0]\n",
    "        \n",
    "    def get_hyper_params_avg_scores(self):\n",
    "        return self._cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization accuracy\n",
    "\n",
    "To estimate the generalization accuracy you will need to use the test-set accuracy. You have already created a code to use a model to do predicitons and calculate the accuracy in TP2 so you only need to re-use it in this TP (and the exam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \n",
    "    RANDOM_STATE = 123\n",
    "    \n",
    "    def __init__(self, in_data, out_data, hyper_param, clf_callback, n_splits=5):\n",
    "        self.in_data=in_data\n",
    "        self.out_data=out_data\n",
    "        self.hyper_param=hyper_param\n",
    "        self.clf_callback=clf_callback\n",
    "        self.n_splits=n_splits\n",
    "        self.out_test=list()\n",
    "        self.pred_test=list()\n",
    "    \n",
    "    def run(self):\n",
    "        # Use the learned model f to do predictions\n",
    "        k_fold = KFold(n_splits=self.n_splits, random_state=self.RANDOM_STATE, shuffle=True)\n",
    "\n",
    "        avg_test_accuracy = 0\n",
    "        \n",
    "        print(\"Test accuracies for each test set\")\n",
    "        print(\"---------------------------------\")\n",
    "        for train_idx, test_idx in k_fold.split(self.in_data, self.out_data):\n",
    "            in_train = self.in_data.iloc[train_idx]\n",
    "            out_train = self.out_data.iloc[train_idx]\n",
    "            in_test = self.in_data.iloc[test_idx]\n",
    "            self.out_test = self.out_data.iloc[test_idx]\n",
    "\n",
    "            clf = self.clf_callback(self.hyper_param)\n",
    "            clf.fit(in_train, out_train)\n",
    "\n",
    "            # Prediction for all test data\n",
    "            if isinstance(clf, DefaultClassifier):\n",
    "                self.pred_test = clf.predict(self.out_test)\n",
    "            else:\n",
    "                self.pred_test = clf.predict(in_test)\n",
    "\n",
    "            # Get test set accuracy        \n",
    "            test_accuracy = self.calculate_pred_accuracy(self.pred_test, self.out_test)\n",
    "            print('Test accuracy', test_accuracy)\n",
    "            avg_test_accuracy += test_accuracy\n",
    "\n",
    "        # Use test sets to get estimate of generalization error (accuracy)\n",
    "        print()\n",
    "        print(f'The test average accuracy is {avg_test_accuracy/self.n_splits}')\n",
    "        return (avg_test_accuracy/self.n_splits)\n",
    " \n",
    "    def get_test_data(self):\n",
    "        return (self.out_test, self.pred_test)\n",
    "    \n",
    "    def calculate_pred_accuracy(self, pred_data, out_data):\n",
    "        match_test = (pred_data == out_data)\n",
    "        accuracy = match_test.sum() / match_test.count()\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test default classifier\n",
    "\n",
    "Default classifier has no hyper-parameters, so you can skip the inner-cross validatoin procedure.\n",
    "\n",
    "**Calculate and report the test accuracy for the default classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracies for each test set\n",
      "---------------------------------\n",
      "Test accuracy 0.5118817109663791\n",
      "Test accuracy 0.5121478873239437\n",
      "Test accuracy 0.5184859154929577\n",
      "Test accuracy 0.5126760563380282\n",
      "Test accuracy 0.5235915492957747\n",
      "\n",
      "The test average accuracy is 0.5157566238834167\n"
     ]
    }
   ],
   "source": [
    "dc_me = ModelEvaluator(\n",
    "    norm_in, \n",
    "    out_autos, \n",
    "    hyper_param=0,\n",
    "    clf_callback=ClassifierCallbacks.DEFAULT\n",
    ")\n",
    "\n",
    "dc_generalization_accuracy = dc_me.run()\n",
    "dc_pred_test, dc_out_test = dc_me.get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New in TP3\n",
    "\n",
    "## Train and test Naive Bayes (NB) classifier\n",
    "\n",
    "All of the above steps are just re-using your work from TP2. Here begins the real added value of TP3.\n",
    "\n",
    "You will need to implement the NB classifier. This will show that you really understand how the method works. The NB classifier is based on basic probability rules such as conditional and joint probability that we have seen in the beginning of the course, practiced in TP1 and reviewed later.\n",
    "\n",
    "### Implement the NB classifier\n",
    "\n",
    "We discussed the Naive Bayes classifier in *Course 11 - 01 Naive Bayes algoritm* so you will need to review the lecture to be able to implement the algorithm. The outline of the implementation steps was at the end of that lecture.\n",
    "\n",
    "### A few more hints:\n",
    "\n",
    "At **training** step of the NB algorithm you use the training data to calculate\n",
    "* the prior probabilities $P(c_i)$ for each output class $c_i, \\, i=1,2$\n",
    "* conditional probabilities $P(x_j \\, | \\, c_i)$ for all discrete attributes and each output class\n",
    " * **hint 1:** use the pseudo-counts explained in *Course 11 - 01 Naive Bayes algoritm*\n",
    " * **hint 2:** to be sure you have all possible values $x_j$ for all discrete attributes get the possible unique values from the full dataset not just the trianing set.  \n",
    " Note: if your dataset is big this should not matter. This is just to make sure that you do not have a value $x_j$ in test that you haven't seen in training and therefore haven't calcualted $P(x_j \\, | \\, c_i)$ for it.\n",
    "* conditional means and variances for all continuous attributes and each output class\n",
    "\n",
    "At **prediction** step of the NB algorithm, for each instance you want to predict you need to calculate \n",
    "* the conditional probabilities $P(x_j \\, | \\, c_i)$ of all the continous attributes and each output class (using the Normal distribution with means and variances calculate over the trianing data above)\n",
    "* the likelihood as the product $P(\\mathbf{x} \\, | \\, c_i) = \\prod_{j=1}^d P(x_j \\, | \\, c_i)$ across all attributes and for each output class $c_i, \\, i=1,2$\n",
    "* the simplified posterior $P(c_i \\, | \\, \\mathbf{x}) \\propto P(\\mathbf{x} \\, | \\, c_i) P(c_i)$\n",
    "\n",
    "Finally, for each instance individually you use the Bayes decision rule: pick the class $c_1$ or $c_2$ which has higher posterior probability (is $P(c_1 \\, | \\, \\mathbf{x})$ higher or smaller than $P(c_2 \\, | \\, \\mathbf{x})$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to follow to implement the Naive Bayes\n",
    "\n",
    "1. calculate probabilities $P(c_i)$ for each output class - Course 3 `count_values()`\n",
    "2. calcluate conditional probabilities $P(x_j \\, | \\, c_i)$ for each output class  \n",
    " 2.1 for all **discrecte attributes** pseudo-counts - Course 3 `crosstab`  \n",
    " 2.2 for all **continous attributes** conditoinal mean/variance and normal probability distribution - Course 3 'groupby` processing, HW1  \n",
    "3. use for-loop to calculate the product $\\prod_{j=1}^d P(x_j \\, | \\, c_i)$\n",
    "4. Use the simplified form $P(c_i \\, | \\, \\mathbf{x}) \\propto P(\\mathbf{x} \\, | \\, c_i) P(c_i)$\n",
    "5. Use Bayes decision rule $\\ \\hat{y} = argmax_{c_i} \\, P(c_i \\, | \\, \\mathbf{x}) $ to predict the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import ClassifierMixin\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier(ClassifierMixin):\n",
    "    \"\"\"Naive Bayes Classifier\n",
    "    \n",
    "    The Naive Bayes Classifier technique is based on the so-called \n",
    "    Bayesian theorem and is particularly suited when the dimensionality \n",
    "    of the inputs is high. Despite its simplicity, Naive Bayes can often \n",
    "    outperform more sophisticated classification methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit Gaussian Naive Bayes according to X, y\n",
    "        \n",
    "            1. calculate probabilities  P(ci) for each output class\n",
    "            2. calcluate conditional probabilities  P(xj|ci) for each output class\n",
    "                2.1 for all discrecte attributes pseudo-counts\n",
    "                2.2 for all continous attributes conditoinal mean/variance and normal probability distribution\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Perform classification on an array of test vectors X.\n",
    "            1. Use the simplified form  P(ci|x)âˆP(x|ci)P(ci)\n",
    "            2. Use Bayes decision rule   Å· =argmaxciP(ci|x)to predict the class. \n",
    "        \"\"\"\n",
    "        jll = 0 # jll is the joint log likelyhood and is calculated in the step 1.\n",
    "        return np.argmax(jll, axis=1) # argmax is computed and is the step 2.\n",
    "    \n",
    "    \n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"\n",
    "            1. use for-loop to calculate the product  âˆdj=1P(xj|ci)\n",
    "        \"\"\"\n",
    "        joint_log_likelihood = []\n",
    "\n",
    "        # for-loop goes here\n",
    "        \n",
    "        joint_log_likelihood = np.array(joint_log_likelihood).T\n",
    "        return joint_log_likelihood\n",
    "    \n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"Return log-probability estimates for the test vector X.\"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probability estimates for the test vector X.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Note:** The Naive Bayes classifier has no hyper-parameters to be selected, therefore you do not need to perform the inner cross-validation.  \n",
    "In this respect the NB classifier is easy.  \n",
    "You only need to do the train/test split and perform the train and prediction steps described above.</font>\n",
    "\n",
    "**Calcualte and report the test accuracy of the NB classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracies for each test set\n",
      "---------------------------------\n",
      "Test accuracy 0.5222672064777328\n",
      "Test accuracy 0.5327464788732394\n",
      "Test accuracy 0.5369718309859155\n",
      "Test accuracy 0.5299295774647887\n",
      "Test accuracy 0.5387323943661971\n",
      "\n",
      "The test average accuracy is 0.5321294976335746\n"
     ]
    }
   ],
   "source": [
    "nb_clf_me = ModelEvaluator(\n",
    "    norm_in, \n",
    "    out_autos, \n",
    "    hyper_param=0,\n",
    "    clf_callback=ClassifierCallbacks.NAIVE_BAYES\n",
    ")\n",
    "\n",
    "nb_clf_generalization_accuracy = nb_clf_me.run() \n",
    "nb_pred_test, nb_out_test = nb_clf_me.get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test logistic regression\n",
    "\n",
    "We discussed the theory of logistic regression in the course *Course 12 - 02 Logistic regression*.\n",
    "\n",
    "Implementing logistic regression from scratch can get somewhat tedious.\n",
    "Therefore we recommend you use an existing implementation in **sci-kit learn**\n",
    "[sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "You can use the official documentation or any other information you can find (google) to make it work correctly.\n",
    "\n",
    "The sci-kit learn implementation of the logistic regression performs the optimisation steps for you, therefore you do **not need to implement the gradient descent** procedure.\n",
    "The general steps for using the logistic regression model in sci-kit learn are the same as se used for decision trees and nearest neighbour and are described **at the end of *Course 12 - 02 Logistic regression***.\n",
    "\n",
    "*Though implementing logistic regression from scratch is rather more demanding, you should in fact be able to do it based on the information provided in the course sheet. If someone wants to give it a try let us know and we will help you get started.*\n",
    "\n",
    "**We want you to use:**\n",
    "* $\\ell_2$ regularization\n",
    "* perform a hyper-parameter search over a grid $\\lambda \\in \\{0.0001, 0.001, 0.01, 0.1, 1, 10, 100 \\}$ using 5-folds inner cross validation (you can change the grid if you wish to achieve better prediciton accuracy, let us know if you decide to do this). \n",
    "* train final model over the full training data using the best $\\lambda$ (write in your file which value you pick as the best)\n",
    "* **calcualte and report the test accuracy of the final logistic regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal hyper-parameter is 100\n",
      "The optimal hyper-parameter is 100\n",
      "The optimal hyper-parameter is 100\n",
      "The optimal hyper-parameter is 100\n",
      "The optimal hyper-parameter is 100\n",
      "\n",
      "Average score for each hyper-parameter\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.516407866559321,\n",
       " 0.001: 0.5394087450937013,\n",
       " 0.01: 0.552938365968489,\n",
       " 0.1: 0.5616176397364421,\n",
       " 1: 0.5683603050110088,\n",
       " 10: 0.5702264750827106,\n",
       " 100: 0.5706137788830021}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most optimal hyper-parameter is 100\n"
     ]
    }
   ],
   "source": [
    "logr_hpt = HyperParameterTuner(norm_in,\n",
    "                              out_autos, \n",
    "                              hyper_params=[0.0001, 0.001, 0.01, 0.1, 1, 10, 100], \n",
    "                              clf_callback=ClassifierCallbacks.LOGISTIC_REGRESSION)\n",
    "logr_hpt.run()\n",
    "\n",
    "print()\n",
    "print(\"Average score for each hyper-parameter\")\n",
    "print(\"--------------------------------------\")\n",
    "display(logr_hpt.get_hyper_params_avg_scores())\n",
    "\n",
    "best_hp_logr = logr_hpt.get_best_hyper_param()\n",
    "print(f'The most optimal hyper-parameter is {best_hp_logr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracies for each test set\n",
      "---------------------------------\n",
      "Test accuracy 0.5645132899137476\n",
      "Test accuracy 0.571830985915493\n",
      "Test accuracy 0.5866197183098592\n",
      "Test accuracy 0.577112676056338\n",
      "Test accuracy 0.5653169014084507\n",
      "\n",
      "The test average accuracy is 0.5730787143207776\n"
     ]
    }
   ],
   "source": [
    "logr_clf_me = ModelEvaluator(\n",
    "    norm_in, \n",
    "    out_autos, \n",
    "    hyper_param=best_hp_logr,\n",
    "    clf_callback=ClassifierCallbacks.LOGISTIC_REGRESSION\n",
    ")\n",
    "\n",
    "logr_clf_generalization_accuracy = logr_clf_me.run()\n",
    "logr_pred_test, logr_out_test = logr_clf_me.get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models\n",
    "\n",
    "Once you have the test accuracies for the Naive Bayes, logistic regression and default classifier, calculate the confidence intervals for the generalization accuracy for each of the algorithm at *95%* confidence level (*Course 10 - 01 Confidence intervals*).\n",
    "\n",
    "This step is similar to the McNemar test. If the intervals of two algorithms overlap, you cannot conclude that one is better than the other (with the given confidence).\n",
    "\n",
    "**Is any of the three algorithms clearly better than the other two based on the generalization accuracy confidence intervals?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "P(0.5191531221699747 <= 0.5321294976335746 <= 0.5451058730971746) = 0.95\n",
      "\n",
      "Logistic regression\n",
      "P(0.560215101887784 <= 0.5730787143207776 <= 0.5859423267537712) = 0.95\n",
      "\n",
      "Default classifier\n",
      "P(0.5027598322873742 <= 0.5157566238834167 <= 0.5287534154794592) = 0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def confidence_interval(test_accuracy, test_size, z=1.96, confidence_level=0.95):\n",
    "    accuracy = test_accuracy\n",
    "    n = test_size\n",
    "    variance = accuracy * (1 - accuracy) / n\n",
    "    standard_deviation = variance**0.5\n",
    "    conf_int_left = accuracy - z * standard_deviation;\n",
    "    conf_int_right = accuracy + z * standard_deviation;\n",
    "    print(f'P({conf_int_left} <= {accuracy} <= {conf_int_right}) = {confidence_level}')\n",
    "    print()\n",
    "\n",
    "print('Naive Bayes')\n",
    "confidence_interval(nb_clf_generalization_accuracy, len(nb_pred_test)) \n",
    "print('Logistic regression')\n",
    "confidence_interval(logr_clf_generalization_accuracy, len(logr_pred_test)) \n",
    "print('Default classifier')\n",
    "confidence_interval(dc_generalization_accuracy, len(dc_pred_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "- Yes, the Logistical Regression algorithm is clearly the best.\n",
    "- However, we cannot conclude that Naive Bayes is better than Default Classifier and vice versa (with the given confidence), because the intervals overlap.\n",
    "- See above numbers for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and recall\n",
    "\n",
    "In *Course 10 - 02 Classification performance measures* we discussed alternative measures for the performance of an algorithm. **Calcualte and report the precision and recall (over the test data) of all the three algorithms of this TP.**\n",
    "\n",
    "**Looking at these, does any of the algoritm look better/worse than the others? Why? Explain, discuss.** (There is no correct or wrong answer, we want to see you understand the concepts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default classifier\n",
      "recall score : 0.5235915492957747\n",
      "precision score : 1.0\n",
      "\n",
      "Logistic regression\n",
      "recall score : 0.5648267008985879\n",
      "precision score : 0.7397444519166106\n",
      "\n",
      "Naive Bayes\n",
      "recall score : 0.5362556329373208\n",
      "precision score : 0.8802958977807667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "print(\"Default classifier\")\n",
    "print(f'recall score : {recall_score(dc_out_test, dc_pred_test)}')\n",
    "print(f'precision score : {precision_score(dc_out_test, dc_pred_test)}')\n",
    "print(\"\")\n",
    "print(\"Logistic regression\")\n",
    "print(f'recall score : {recall_score(logr_out_test, logr_pred_test)}')\n",
    "print(f'precision score : {precision_score(logr_out_test, logr_pred_test)}')\n",
    "print(\"\")\n",
    "print(\"Naive Bayes\")\n",
    "print(f'recall score : {recall_score(nb_out_test, nb_pred_test)}')\n",
    "print(f'precision score : {precision_score(nb_out_test, nb_pred_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "- To fully evaluate the effectiveness of a model, we must examine both precision and recall. Unfortunately, precision and recall are often in tension. That is, improving precision typically reduces recall and vice versa.\n",
    "- In our case, precision measures the percentage of cars flagged as 'fast_sale' that were correctly classified. \n",
    "- In our case, recall measures the percentage of actual 'fast_sale' cars that were correctly classified.\n",
    "\n",
    "#### Default Classifier\n",
    "- This model has a precision of 1.0 â€” in other words, when it predicts a car is 'fast_sale', it is correct 100% of the time.\n",
    "- This is theorically true, but due to the nature of the model â€”it puts all the prediction data to true or false (depending on the majority class)â€” the precision is biased in practice.\n",
    "- This model has a recall of 0.5235915492957747 â€” in other words, it correctly identifies ~52% of all cars thar are 'fast_sale'.\n",
    "\n",
    "#### Logistic regression\n",
    "- This model has a precision of 0.7397444519166106 â€” in other words, when it predicts a car is 'fast_sale', it is correct ~73% of the time.\n",
    "- This model has a recall of 0.5648267008985879 â€” in other words, it correctly identifies ~56% of all cars thar are 'fast_sale'.\n",
    "\n",
    "####  Naive Bayes\n",
    "- This model has a precision of 0.8802958977807667 â€” in other words, when it predicts a car is 'fast_sale', it is correct 88% of the time.\n",
    "- This model has a recall of 0.5362556329373208 â€” in other words, it correctly identifies ~53% of all cars thar are 'fast_sale'.\n",
    "\n",
    "#### Conclusion\n",
    "- Precision can be seen as a measure of exactness or quality, whereas recall is a measure of completeness or quantity. \n",
    "- High precision means that an algorithm returned substantially more relevant results than irrelevant ones.\n",
    "- High recall means that an algorithm returned most of the relevant results.\n",
    "- Without a context, we can't say that an algorighm looks better or worse than the other as it depends of what we are looking for (exactness or completness)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
